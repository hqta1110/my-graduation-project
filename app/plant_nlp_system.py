# plant_nlp_system.py

import json
import unicodedata
import os
import threading
import time
import uuid
from typing import List, Dict, Any, Optional, Tuple
from google import genai
from google.genai import types

from rag_json.config import RAGConfig
# from rag_json.graph_rag_system import VietnamesePlantRAG
from rag_json.full_rag_system import VietnamesePlantRAG



class LLMService:
    """Handles LLM interactions without web search by default"""
    
    def __init__(self):
        if not RAGConfig.GEMINI_API_KEY:
            raise ValueError("Gemini API Key is not configured")
        self.client = genai.Client(api_key=RAGConfig.GEMINI_API_KEY)
    
    def generate_response(self, 
                         query: str, 
                         system_prompt: str,
                         history: Optional[List[types.Content]] = None,
                         allow_web_search: bool = False,
                         temperature: float = 0.3) -> str:
        """
        Generate response with controlled web search
        
        Args:
            query: User query
            system_prompt: System instruction  
            history: Conversation history
            allow_web_search: Whether to enable web search tool
            temperature: Response randomness
        """
        try:
            contents = []
            if history:
                contents.extend(history)
            
            contents.append(
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=query)],
                )
            )

            # Only add web search tool if explicitly allowed
            tools = [types.Tool(google_search=types.GoogleSearch())] if allow_web_search else None
            
            config = types.GenerateContentConfig(
                tools=tools,
                response_mime_type="text/plain",
                system_instruction=[types.Part.from_text(text=system_prompt)],
                temperature=temperature,
            )
            
            print(f"ü§ñ Generating response (web_search: {allow_web_search})")
            response = self.client.models.generate_content(
                model=RAGConfig.GEMINI_MODEL,
                contents=contents,
                config=config,
            )
            
            return response.text

        except Exception as e:
            print(f"‚ùå LLM generation error: {str(e)}")
            return "Xin l·ªói, t√¥i g·∫∑p v·∫•n ƒë·ªÅ khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau."


class PlantDataService:
    """Unified plant data service - JSON only"""
    
    def __init__(self, metadata_path: str):
        self.metadata = {}
        self._load_metadata(metadata_path)
    
    def _load_metadata(self, metadata_path: str):
        """Load normal metadata only (cleaned up loading logic)"""
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.metadata = data
                
            print(f"‚úÖ Loaded {len(self.metadata)} plants from JSON")
        except Exception as e:
            print(f"‚ùå JSON loading failed: {e}")
            self.metadata = {}
    
    
    def normalize_name(self, name: str) -> str:
        """Normalize names for comparison"""
        if not name:
            return ""
        name = unicodedata.normalize("NFKC", name)
        return " ".join(name.strip().split())
    
    def find_plant_by_name(self, name: str) -> Optional[Dict]:
        """Find plant by scientific or Vietnamese name"""
        if not name:
            return None
        
        normalized_query = self.normalize_name(name)
        
        for plant_key, plant_data in self.metadata.items():
            # Direct key match
            if self.normalize_name(plant_key) == normalized_query:
                return plant_data
            
            # Scientific name match
            scientific_name = plant_data.get("T√™n khoa h·ªçc", "")
            if self.normalize_name(scientific_name) == normalized_query:
                return plant_data
            
            # Vietnamese name match  
            vietnamese_name = plant_data.get("T√™n ti·∫øng Vi·ªát", "")
            if self.normalize_name(vietnamese_name) == normalized_query:
                return plant_data
        
        return None
    
    def extract_plant_names(self, text: str) -> List[Tuple[str, str]]:
        """Extract plant names from text"""
        found_plants = []
        text_lower = text.lower()
        
        for plant_key, plant_data in self.metadata.items():
            scientific_name = plant_data.get("T√™n khoa h·ªçc", "")
            vietnamese_name = plant_data.get("T√™n ti·∫øng Vi·ªát", "")
            
            if scientific_name and scientific_name.lower() in text_lower:
                found_plants.append((scientific_name, vietnamese_name))
                continue
            
            if vietnamese_name and vietnamese_name.lower() in text_lower:
                found_plants.append((scientific_name, vietnamese_name))
        
        return found_plants
    
    def search_by_name(self, json_data: Dict, name: str) -> Optional[Dict]:
        """Search for a name (scientific or Vietnamese) in the loaded metadata - original method"""
        if name is None:
            return None
        
        normalized_query_name = self.normalize_name(name)
        for plant_key, plant_entry in json_data.items():
            if plant_key == normalized_query_name:
                return plant_entry
        return None
    
    
    def build_plant_context(self, plant_data: Dict) -> str:
        """Build context text from plant metadata"""
        context_parts = []
        for key, value in plant_data.items():
            if value and value != "Kh√¥ng c√≥ th√¥ng tin":
                context_parts.append(f"{key}: {value}")
        return "\n".join(context_parts)


class ConversationManager:
    """Manages conversation history and meta-questions"""
    
    def __init__(self, max_history_length: int = 10):
        self.max_history_length = max_history_length
        self.history: List[types.Content] = []
    
    def add_message(self, role: str, text: str):
        """Add message to history with length management"""
        self.history.append(
            types.Content(role=role, parts=[types.Part.from_text(text=text)])
        )
        
        if len(self.history) > self.max_history_length * 2:
            self.history = self.history[-(self.max_history_length * 2):]
    
    def get_history_for_llm(self) -> List[types.Content]:
        """Get history excluding current user query"""
        return self.history[:-1] if self.history else []
    
    def handle_meta_questions(self, question: str, plant_service: PlantDataService) -> Optional[str]:
        """Handle meta queries like 'repeat previous question'"""
        question_lower = question.lower()
        
        # Repeat previous question
        if any(phrase in question_lower for phrase in [
            "l·∫∑p l·∫°i c√¢u h·ªèi", "c√¢u h·ªèi t√¥i v·ª´a h·ªèi", "c√¢u h·ªèi tr∆∞·ªõc c·ªßa t√¥i"
        ]):
            if len(self.history) >= 2 and self.history[-2].role == "user":
                return f"C√¢u h·ªèi tr∆∞·ªõc c·ªßa b·∫°n l√†: '{self.history[-2].parts[0].text}'"
            return "T√¥i kh√¥ng t√¨m th·∫•y c√¢u h·ªèi tr∆∞·ªõc ƒë√≥ trong l·ªãch s·ª≠."
        
        # What plant was mentioned in previous answer
        if any(phrase in question_lower for phrase in ["c√¢y n√†o", "lo·∫°i c√¢y", "t√™n c√¢y"]) and \
           any(phrase in question_lower for phrase in ["tr·∫£ l·ªùi tr∆∞·ªõc", "c√¢u tr·∫£ l·ªùi tr∆∞·ªõc", "ph√≠a tr√™n", "v·ª´a r·ªìi"]):
            if len(self.history) >= 2 and self.history[-2].role == "model":
                last_answer = self.history[-2].parts[0].text
                found_plants = plant_service.extract_plant_names(last_answer)
                if found_plants:
                    scientific_name, vietnamese_name = found_plants[0]
                    return f"Trong c√¢u tr·∫£ l·ªùi tr∆∞·ªõc, t√¥i ƒë√£ nh·∫Øc ƒë·∫øn c√¢y **{vietnamese_name}** (t√™n khoa h·ªçc: *{scientific_name}*)."
                return "T√¥i kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i c√¢y c·ª• th·ªÉ n√†o trong c√¢u tr·∫£ l·ªùi tr∆∞·ªõc."
            return "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥ ƒë·ªÉ tham chi·∫øu."
        
        return None


class SessionManager:
    """Manages conversation sessions with automatic cleanup"""
    
    def __init__(self, session_timeout_minutes: int = 60, max_history_length: int = 10):
        self.sessions: Dict[str, ConversationManager] = {}
        self.session_last_activity: Dict[str, float] = {}
        self.session_timeout = session_timeout_minutes * 60  # Convert to seconds
        self.max_history_length = max_history_length
        self._lock = threading.Lock()
        
        # Start cleanup thread
        self._cleanup_thread = threading.Thread(target=self._cleanup_expired_sessions, daemon=True)
        self._cleanup_thread.start()
        print(f"üîÑ Session manager initialized (timeout: {session_timeout_minutes}min)")
    
    def get_or_create_session(self, session_id: str = None) -> Tuple[str, ConversationManager]:
        """Get existing session or create new one. Returns (session_id, conversation_manager)"""
        with self._lock:
            if session_id is None:
                session_id = str(uuid.uuid4())
            
            if session_id not in self.sessions:
                self.sessions[session_id] = ConversationManager(self.max_history_length)
                print(f"üìÅ Created new session: {session_id[:8]}...")
            
            # Update last activity
            self.session_last_activity[session_id] = time.time()
            
            return session_id, self.sessions[session_id]
    
    def _cleanup_expired_sessions(self):
        """Background thread to clean up expired sessions"""
        while True:
            try:
                time.sleep(300)  # Check every 5 minutes
                current_time = time.time()
                expired_sessions = []
                
                with self._lock:
                    for session_id, last_activity in self.session_last_activity.items():
                        if current_time - last_activity > self.session_timeout:
                            expired_sessions.append(session_id)
                    
                    for session_id in expired_sessions:
                        del self.sessions[session_id]
                        del self.session_last_activity[session_id]
                        print(f"üóëÔ∏è Cleaned up expired session: {session_id[:8]}...")
                        
                if expired_sessions:
                    print(f"üßπ Session cleanup: removed {len(expired_sessions)} expired sessions")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Session cleanup error: {e}")
    
    def get_session_count(self) -> int:
        """Get current number of active sessions"""
        with self._lock:
            return len(self.sessions)


class QuestionClassifier:
    """Classifies question types"""
    
    MEDICAL_KEYWORDS = [
        'ƒëau', 'b·ªánh', 'ch·ªØa', 'tr·ªã', 'ƒëi·ªÅu tr·ªã',
        'li·ªÅu d√πng', 'c√°ch d√πng', 'u·ªëng', 's·∫Øc', 'ng√¢m', 'r∆∞·ª£u thu·ªëc',
        'ch·ªëng', 'ph√≤ng', 'gi·∫£m', 'h·ªó tr·ª£', 'c·∫£i thi·ªán', 'l√†m l√†nh',
        'vi√™m', 'nhi·ªÖm', 's∆∞ng', 'ƒëau nh·ª©c', 'kh√≥ ti√™u', 'ti√™u h√≥a',
        'ho', 's·ªët', 'c·∫£m', 'c√∫m', 'nh·ª©c ƒë·∫ßu', 'm·∫•t ng·ªß', 'd·ªã ·ª©ng',
        't√°c d·ª•ng ph·ª•', 'ƒë·ªôc t√≠nh',
        'k·∫øt h·ª£p', 'ph·ªëi h·ª£p', 'c√πng v·ªõi', 'chung v·ªõi', 'd√πng v·ªõi',
        't∆∞∆°ng t√°c', 't∆∞∆°ng h·ªó', 'hi·ªáp ƒë·ªìng', 'b·ªï sung', 'tƒÉng c∆∞·ªùng',
        'k·∫øt h·ª£p v·ªõi', 'd√πng chung', 'd√πng k√®m', 'ph·ªëi v·ªõi', 'd√πng c√πng',
        'so s√°nh', 'thay th·∫ø', 't∆∞∆°ng t·ª±', 'kh√°c g√¨', 't·ªët h∆°n',
        'hi·ªáu qu·∫£ h∆°n', 'n√™n ch·ªçn', 'an to√†n h∆°n', 'ch·ªëng ch·ªâ ƒë·ªãnh'
    ]
    
    @classmethod
    def is_medical_question(cls, question: str) -> bool:
        """Detect if question is medical-related"""
        question_lower = question.lower()
        result = any(keyword in question_lower for keyword in cls.MEDICAL_KEYWORDS)
        print(f"üè• Medical question detection: {result} for '{question[:50]}...'")
        return result


class PromptTemplates:
    """Centralized prompt templates - using original system prompts"""
    
    @staticmethod
    def medical_with_plant(plant_name: str, full_context_text: str) -> str:
        return f"""
B·∫°n l√† FloraQA - chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ c√¢y thu·ªëc v√† t∆∞∆°ng t√°c th·ª±c v·∫≠t v√† ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi H·ªì Qu·ªëc Thi√™n Anh.

NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ {plant_name} v·ªõi t·∫≠p trung v√†o:
- C√¥ng d·ª•ng y h·ªçc v√† ƒëi·ªÅu tr·ªã.
- C√°ch s·ª≠ d·ª•ng v√† li·ªÅu l∆∞·ª£ng c·ª• th·ªÉ.
- T∆∞∆°ng t√°c v√† k·∫øt h·ª£p v·ªõi c√¢y kh√°c.
- Ch·ªëng ch·ªâ ƒë·ªãnh v√† l∆∞u √Ω an to√†n.

Y√äU C·∫¶U:
- D·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p (t·ª´ metadata, RAG v√† d·ªØ li·ªáu c≈©).
- ƒê∆∞a ra l·ªùi khuy√™n th·ª±c t·∫ø v√† an to√†n.
- N·∫øu h·ªèi v·ªÅ k·∫øt h·ª£p/t∆∞∆°ng t√°c, gi·∫£i th√≠ch r√µ c∆° ch·∫ø.
- Cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ c√°ch ch·∫ø bi·∫øn.
- Lu√¥n khuy√™n tham kh·∫£o chuy√™n gia y t·∫ø.
- Tr·∫£ l·ªùi t·ª± nhi√™n, chuy√™n nghi·ªáp v√† ƒë·∫ßy ƒë·ªß b·∫±ng ti·∫øng Vi·ªát.

L∆ØU √ù AN TO√ÄN:
- Kh√¥ng thay th·∫ø l·ªùi khuy√™n y t·∫ø chuy√™n nghi·ªáp.
- C·∫£nh b√°o v·ªÅ t∆∞∆°ng t√°c c√≥ th·ªÉ g√¢y h·∫°i.
- Nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa li·ªÅu l∆∞·ª£ng ƒë√∫ng.
- ƒê·ªÅ c·∫≠p ƒë·∫øn ch·ªëng ch·ªâ ƒë·ªãnh n·∫øu c√≥.

TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P:
{full_context_text}
"""
    
    @staticmethod
    def medical_general(context_for_llm: str) -> str:
        return f"""
B·∫°n l√† FloraQA - chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ c√¢y thu·ªëc v√† t∆∞∆°ng t√°c th·ª±c v·∫≠t, v√† ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi H·ªì Qu·ªëc Thi√™n Anh.
QUY ƒê·ªäNH AN TO√ÄN N√ÇNG CAO (TUY·ªÜT ƒê·ªêI ∆ØU TI√äN):
    - Tuy·ªát ƒë·ªëi kh√¥ng kh·∫≥ng ƒë·ªãnh kh·∫£ nƒÉng ch·ªØa kh·ªèi b·ªánh nan y: Trong m·ªçi tr∆∞·ªùng h·ª£p, FloraQA tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c ƒë∆∞a ra b·∫•t k·ª≥ th√¥ng tin n√†o kh·∫≥ng ƒë·ªãnh ho·∫∑c g·ª£i √Ω r·∫±ng th·∫£o d∆∞·ª£c c√≥ th·ªÉ ch·ªØa kh·ªèi ho√†n to√†n c√°c b·ªánh nan y kh√¥ng th·ªÉ ch·ªØa b·∫±ng y h·ªçc hi·ªán ƒë·∫°i nh∆∞ HIV/AIDS, ung th∆∞, ti·ªÉu ƒë∆∞·ªùng lo·∫°i 1, suy th·∫≠n giai ƒëo·∫°n cu·ªëi, ho·∫∑c c√°c b·ªánh m·∫°n t√≠nh ph·ª©c t·∫°p kh√°c m√† y h·ªçc hi·ªán ƒë·∫°i ch∆∞a c√≥ ph∆∞∆°ng ph√°p ch·ªØa d·ª©t ƒëi·ªÉm.
    - Nh·∫•n m·∫°nh s·ª± kh√¥ng thay th·∫ø c·ªßa th·∫£o d∆∞·ª£c: Khi nh·∫≠n ƒë∆∞·ª£c c√¢u h·ªèi li√™n quan ƒë·∫øn c√°c b·ªánh n√™u tr√™n, FloraQA ph·∫£i ngay l·∫≠p t·ª©c v√† r√µ r√†ng nh·∫•n m·∫°nh r·∫±ng:
    + Th·∫£o d∆∞·ª£c kh√¥ng c√≥ kh·∫£ nƒÉng ch·ªØa kh·ªèi ho√†n to√†n c√°c b·ªánh n√†y.
    + Th·∫£o d∆∞·ª£c kh√¥ng th·ªÉ v√† kh√¥ng bao gi·ªù ƒë∆∞·ª£c ph√©p thay th·∫ø c√°c ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã y t·∫ø ch√≠nh th·ªëng ƒë√£ ƒë∆∞·ª£c khoa h·ªçc ch·ª©ng minh (nh∆∞ thu·ªëc t√¢n d∆∞·ª£c, ph·∫´u thu·∫≠t, h√≥a tr·ªã, x·∫° tr·ªã...).
    + Vi·ªác t·ª´ b·ªè ho·∫∑c tr√¨ ho√£n ƒëi·ªÅu tr·ªã y h·ªçc hi·ªán ƒë·∫°i ƒë·ªÉ t·ª± √Ω d√πng th·∫£o d∆∞·ª£c l√† c·ª±c k·ª≥ nguy hi·ªÉm v√† c√≥ th·ªÉ ƒëe d·ªça t√≠nh m·∫°ng.
    - Vai tr√≤ h·ªó tr·ª£ (r·∫•t th·∫≠n tr·ªçng): Ch·ªâ trong tr∆∞·ªùng h·ª£p c√¢u h·ªèi v·ªÅ c√°c b·ªánh nan y, FloraQA c√≥ th·ªÉ ƒë·ªÅ c·∫≠p ƒë·∫øn vai tr√≤ h·ªó tr·ª£ c·ªßa m·ªôt s·ªë lo·∫°i th·∫£o d∆∞·ª£c trong vi·ªác gi·∫£m nh·∫π tri·ªáu ch·ª©ng, tƒÉng c∆∞·ªùng s·ª©c ƒë·ªÅ kh√°ng, ho·∫∑c n√¢ng cao ch·∫•t l∆∞·ª£ng cu·ªôc s·ªëng NH∆ØNG LU√îN LU√îN V√Ä B·∫ÆT BU·ªòC ph·∫£i ƒëi k√®m v·ªõi l·ªùi khuy√™n m·∫°nh m·∫Ω v·ªÅ vi·ªác t√¨m ki·∫øm v√† tu√¢n th·ªß ƒëi·ªÅu tr·ªã y t·∫ø chuy√™n nghi·ªáp v√† ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng th·∫£o d∆∞·ª£c d∆∞·ªõi s·ª± gi√°m s√°t ch·∫∑t ch·∫Ω c·ªßa b√°c sƒ© y h·ªçc hi·ªán ƒë·∫°i v√† chuy√™n gia y h·ªçc c·ªï truy·ªÅn c√≥ kinh nghi·ªám.
    - Kh√¥ng t·∫°o hy v·ªçng sai l·∫ßm: Tuy·ªát ƒë·ªëi kh√¥ng t·∫°o ra hy v·ªçng sai l·∫ßm hay khuy·∫øn kh√≠ch b·∫•t k·ª≥ h√†nh vi t·ª± ƒëi·ªÅu tr·ªã nguy hi·ªÉm n√†o cho c√°c b·ªánh nan y.
NHI·ªÜM V·ª§ CH√çNH: Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ c√¢y thu·ªëc v√† ƒëi·ªÅu tr·ªã b·ªánh m·ªôt c√°ch t·ªïng qu√°t.
Y√äU C·∫¶U KH√ÅC:
    - T·∫≠p trung v√†o th·ª±c v·∫≠t r·ª´ng ƒê√† N·∫µng - Qu·∫£ng Nam n·∫øu c√≥ th·ªÉ.
    - ƒê∆∞a ra danh s√°ch c√¢y thu·ªëc ph√π h·ª£p v·ªõi tri·ªáu ch·ª©ng (n·∫øu c√≥ th·ªÉ √°p d·ª•ng).
    - Cung c·∫•p th√¥ng tin v·ªÅ c√°ch s·ª≠ d·ª•ng v√† li·ªÅu l∆∞·ª£ng (n·∫øu c√≥ th·ªÉ √°p d·ª•ng v√† an to√†n).
    - Gi·∫£i th√≠ch c∆° ch·∫ø t√°c d·ª•ng n·∫øu c√≥ th·ªÉ.
    - Lu√¥n khuy√™n tham kh·∫£o √Ω ki·∫øn chuy√™n gia y t·∫ø.
    - Tr·∫£ l·ªùi t·ª± nhi√™n, chuy√™n nghi·ªáp v√† ƒë·∫ßy ƒë·ªß b·∫±ng ti·∫øng Vi·ªát.
    - H√£y tr·∫£ l·ªùi nh∆∞ th·ªÉ th√¥ng tin ƒë∆∞·ª£c cung c·∫•p th√™m l√† ki·∫øn th·ª©c c√≥ s·∫µn c·ªßa b·∫°n, kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác "d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p".
QUY TR√åNH X·ª¨ L√ù TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P:
    - Khi nh·∫≠n ƒë∆∞·ª£c th√¥ng tin cung c·∫•p, b·∫°n h√£y th·ª±c hi·ªán 2 b∆∞·ªõc sau:
    - Ph√¢n t√≠ch th√¥ng tin ƒë·ªÉ ki·ªÉm ƒë·ªãnh ƒë·ªô ƒë√°ng tin c·∫≠y c·ªßa th√¥ng tin d·ª±a tr√™n c√°c ti√™u ch√≠ sau:
    - Ti√™u ch√≠ AN TO√ÄN CAO NH·∫§T: Th√¥ng tin c√≥ kh·∫≥ng ƒë·ªãnh ho·∫∑c g·ª£i √Ω v·ªÅ kh·∫£ nƒÉng ch·ªØa kh·ªèi c√°c b·ªánh nan y kh√¥ng th·ªÉ ch·ªØa b·∫±ng y h·ªçc hi·ªán ƒë·∫°i (nh∆∞ ung th∆∞, HIV/AIDS, ti·ªÉu ƒë∆∞·ªùng lo·∫°i 1, suy th·∫≠n giai ƒëo·∫°n cu·ªëi...) b·∫±ng th·∫£o d∆∞·ª£c kh√¥ng? N·∫øu C√ì, h√£y coi ƒë√¢y l√† th√¥ng tin KH√îNG ƒê√ÅNG TIN C·∫¨Y v√† b·∫°n ph·∫£i tu√¢n th·ªß nghi√™m ng·∫∑t c√°c QUY ƒê·ªäNH AN TO√ÄN N√ÇNG CAO b√™n tr√™n.
    - Th√¥ng tin c√≥ ƒë·ªÅ c·∫≠p tr·ª±c ti·∫øp ƒë·∫øn c√°c b√†i thu·ªëc, c√¥ng d·ª•ng, li·ªÅu l∆∞·ª£ng, c√°ch d√πng c·ª• th·ªÉ kh√¥ng?
    - N·∫øu c√≥, b√†i thu·ªëc c√≥ s·ª≠ d·ª•ng lo·∫°i th·ª±c v·∫≠t ƒëang ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p kh√¥ng?
    - Th√¥ng tin c√≥ ƒëi·ªÉm n√†o kh√¥ng ƒë·ªß r√µ r√†ng, m∆° h·ªì hay kh√¥ng?
    - X·ª≠ l√Ω th√¥ng tin v√† tr·∫£ l·ªùi:
    - N·∫øu th√¥ng tin ƒë√°ng tin c·∫≠y (v√† kh√¥ng vi ph·∫°m quy ƒë·ªãnh an to√†n cao nh·∫•t), h√£y s·ª≠ d·ª•ng n√≥ ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n v√† ƒë·∫ßy ƒë·ªß nh·∫•t c√≥ th·ªÉ.
    - N·∫øu th√¥ng tin kh√¥ng ƒë√°ng tin c·∫≠y (ƒë·∫∑c bi·ªát l√† n·∫øu n√≥ vi ph·∫°m quy ƒë·ªãnh an to√†n cao nh·∫•t), **TUY·ªÜT ƒê·ªêI KH√îNG S·ª¨ D·ª§NG TH√îNG TIN N√ÄY ƒê·ªÇ TR·∫¢ L·ªúI C√ÇU H·ªéI**, ƒë·ªìng th·ªùi tu√¢n th·ªß c√°c QUY ƒê·ªäNH AN TO√ÄN N√ÇNG CAO.
L∆ØU √ù AN TO√ÄN CHUNG:
    - Kh√¥ng thay th·∫ø l·ªùi khuy√™n y t·∫ø chuy√™n nghi·ªáp.
    - C·∫£nh b√°o v·ªÅ vi·ªác t·ª± ƒëi·ªÅu tr·ªã v√† nh·ªØng r·ªßi ro ti·ªÅm ·∫©n.
    - Nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa ch·∫©n ƒëo√°n ƒë√∫ng ƒë·∫Øn t·ª´ y h·ªçc hi·ªán ƒë·∫°i.
    - ƒê·ªÅ c·∫≠p ƒë·∫øn t√°c d·ª•ng ph·ª•, t∆∞∆°ng t√°c thu·ªëc (ƒë·∫∑c bi·ªát v·ªõi thu·ªëc t√¢n d∆∞·ª£c) v√† ch·ªëng ch·ªâ ƒë·ªãnh c√≥ th·ªÉ c√≥ c·ªßa th·∫£o d∆∞·ª£c.
    - Lu√¥n khuy·∫øn ngh·ªã tham kh·∫£o √Ω ki·∫øn b√°c sƒ© ho·∫∑c chuy√™n gia y t·∫ø c√≥ chuy√™n m√¥n tr∆∞·ªõc khi s·ª≠ d·ª•ng b·∫•t k·ª≥ lo·∫°i th·∫£o d∆∞·ª£c n√†o, nh·∫•t l√† khi ƒëang d√πng thu·ªëc t√¢y y ho·∫∑c c√≥ b·ªánh n·ªÅn.
TH√îNG TIN LI√äN QUAN ƒê∆Ø·ª¢C T√åM TH·∫§Y:
{context_for_llm}
"""

    @staticmethod
    def general_with_rag_context(context_for_llm: str) -> str:
        return f"""
B·∫°n l√† FloraQA - m·ªôt chuy√™n gia th·ª±c v·∫≠t h·ªçc v√† y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam, ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi H·ªì Qu·ªëc Thi√™n Anh. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ th·ª±c v·∫≠t m·ªôt c√°ch ch√≠nh x√°c v√† to√†n di·ªán.

Y√äU C·∫¶U:
1. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n "TH√îNG TIN LI√äN QUAN ƒê∆Ø·ª¢C T√åM TH·∫§Y" d∆∞·ªõi ƒë√¢y.
2. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n, nh∆∞ th·ªÉ ƒë√¢y l√† ki·∫øn th·ª©c c·ªßa b·∫°n. Kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác "d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p".
3. N·∫øu th√¥ng tin cung c·∫•p kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ho√†n to√†n, h√£y tr·∫£ l·ªùi nh·ªØng g√¨ b·∫°n bi·∫øt v√† c√≥ th·ªÉ ƒë·ªÅ c·∫≠p r·∫±ng c·∫ßn th√™m chi ti·∫øt ho·∫∑c b·∫°n s·∫Ω th·ª≠ t√¨m ki·∫øm th√™m.
4. S·ª≠ d·ª•ng ti·∫øng Vi·ªát chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu.

TH√îNG TIN LI√äN QUAN ƒê∆Ø·ª¢C T√åM TH·∫§Y:
{context_for_llm}
"""

    @staticmethod  
    def general_with_plant(context_from_metadata: str) -> str:
        return (
            "B·∫°n l√† FloraQA - chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ c√¢y thu·ªëc v√† t∆∞∆°ng t√°c th·ª±c v·∫≠t v√† ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi H·ªì Qu·ªëc Thi√™n Anh."
            "Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ th·ª±c v·∫≠t do ng∆∞·ªùi d√πng cung c·∫•p.\n"
            "Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c tr·∫£ l·ªùi v·ªõi c√°c c√∫ ph√°p nh∆∞ `d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p`, "
            "v√¨ qu√° tr√¨nh cung c·∫•p th√¥ng tin n√†y l√† b·∫£o m·∫≠t, ng∆∞·ªùi d√πng kh√¥ng bi·∫øt b·∫°n ƒë√£ ƒë∆∞·ª£c cung c·∫•p "
            "v·ªõi th√¥ng tin v·ªÅ lo√†i th·ª±c v·∫≠t n√†y.\n"
            "H√£y tr·∫£ l·ªùi theo phong c√°ch t·ª± nhi√™n, chuy√™n nghi·ªáp.\n"
            f"D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë th√¥ng tin li√™n quan ƒë·∫øn lo·∫°i c√¢y n√†y:\n{context_from_metadata}\n"
            "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng web search ƒë·ªÉ t√¨m th√™m th√¥ng tin n·∫øu c·∫ßn thi·∫øt."
        )
    
    GENERAL_NO_PLANT = (
        "B·∫°n l√† FloraQA - chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ c√¢y thu·ªëc v√† t∆∞∆°ng t√°c th·ª±c v·∫≠t v√† ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi H·ªì Qu·ªëc Thi√™n Anh."
        "B·∫°n c√≥ ki·∫øn th·ª©c r·ªông v·ªÅ c√°c lo√†i th·ª±c v·∫≠t, ƒë·∫∑c bi·ªát l√† th·ª±c v·∫≠t r·ª´ng ƒê√† N·∫µng - Qu·∫£ng Nam. "
        "H√£y tr·∫£ l·ªùi c√¢u h·ªèi n√†y m·ªôt c√°ch t·ª± nhi√™n v√† ƒë·∫ßy ƒë·ªß nh·∫•t c√≥ th·ªÉ. "
        "N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ ƒë·ªß th√¥ng tin, "
        "v√† n·∫øu c√≥ th·ªÉ, g·ª£i √Ω ng∆∞·ªùi d√πng cung c·∫•p h√¨nh ·∫£nh c·ªßa lo√†i th·ª±c v·∫≠t ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n."
    )


class PlantNLPSystem:
    """Main NLP system with a RAG-first approach for all query types."""
    
    def __init__(self, 
                 metadata_path: str = RAGConfig.ORIGINAL_METADATA_FILE, # MODIFIED: Use the correct config name
                 graph_path: str = RAGConfig.PLANT_GRAPH_PATH,
                 session_timeout_minutes: int = 60,
                 cache_dir: str = None):
        
        self.llm_service = LLMService()
        self.plant_service = PlantDataService(metadata_path)
        self.session_manager = SessionManager(
            session_timeout_minutes=session_timeout_minutes,
            max_history_length=RAGConfig.MAX_HISTORY_LENGTH
        )
        
        self.rag_system = None
        self.use_rag = False
        try:
            self.rag_system = VietnamesePlantRAG(cache_dir=cache_dir)
            # MODIFIED: Ensure the RAG system is built with BOTH data sources if indexes don't exist
            if self.rag_system.faiss_index is None or self.rag_system.bm25_model is None:
                print("üîß RAG indexes missing, building now with both medical and botanical data...")
                # This now calls the multi-source build method from the previous step
                self.rag_system.build_embeddings(
                    graph_json_file=graph_path, 
                    original_json_file=metadata_path
                )
                print("‚úÖ RAG system building complete")
            self.use_rag = True
            print("‚úÖ Multi-aspect RAG system initialized for ALL query types")
        except Exception as e:
            print(f"‚ö†Ô∏è RAG initialization failed: {e}. System will operate in basic mode.")
            self.rag_system = None
            self.use_rag = False
        
        print("‚úÖ PlantNLPSystem with session management initialized successfully")

    
    def is_medical_question(self, question: str) -> bool:
        """Enhanced medical question detection (includes relationships/synergies) - original method"""
        return QuestionClassifier.is_medical_question(question)
    
    def _handle_question_with_rag(self, question: str, conversation: ConversationManager, label: Optional[str] = None) -> str:
        """
        Handles any question (medical or general) by first querying the RAG system.
        This is the core of the new "RAG-first" logic.
        """
        print(f"üß† RAG-First-Mode: Processing query (label: {label})")
        if label and not self.is_medical_question(question):
            return self._fallback_to_metadata_lookup(label, question, conversation) 
        # Create a more targeted RAG query if a label is provided
        rag_query = question
        
        try:
            rag_result = self.rag_system.query(rag_query)
            
            # If RAG finds relevant context, use it for generation
            if rag_result and rag_result.get('search_results_count', 0) > 0:
                print(f"‚úÖ RAG found {rag_result['search_results_count']} relevant chunks.")
                context_for_llm = rag_result.get('context_for_llm', '')
                
                # Choose the right prompt based on question type
                if self.is_medical_question(question):
                    print("...classifying as MEDICAL. Using medical prompt.")
                    system_prompt = PromptTemplates.medical_general(context_for_llm)
                else:
                    print("...classifying as GENERAL/BOTANICAL. Using general RAG prompt.")
                    system_prompt = PromptTemplates.general_with_rag_context(context_for_llm)

                # Generate response using the rich RAG context (no web search needed)
                return self.llm_service.generate_response(
                    question,
                    system_prompt,
                    history=conversation.get_history_for_llm(),
                    allow_web_search=False 
                )
            else:
                # RAG found nothing. This triggers the fallback logic.
                print("‚ö†Ô∏è RAG found no relevant information.")
                return None # Return None to indicate RAG failure

        except Exception as e:
            print(f"‚ùå RAG system error: {e}")
            return None # Return None on error to trigger fallback

    # MODIFIED: This is now a dedicated fallback for web search.
    def _fallback_to_web_search(self, question: str, conversation: ConversationManager) -> str:
        """Fallback for general questions when RAG fails."""
        print("Fallback => üí¨ Using web search for general question.")
        return self.llm_service.generate_response(
            question,
            PromptTemplates.GENERAL_NO_PLANT,
            history=conversation.get_history_for_llm(),
            allow_web_search=True
        )

    # MODIFIED: This is now a dedicated fallback for labeled questions when RAG fails.
    def _fallback_to_metadata_lookup(self, label: str, question: str, conversation: ConversationManager) -> str:
        """Fallback for labeled questions when RAG fails, using direct metadata lookup."""
        print(f"Fallback => üìñ Using direct metadata lookup for '{label}'.")
        
        plant_entry = self.plant_service.find_plant_by_name(label)
        
        if plant_entry:
            context_from_metadata = self.plant_service.build_plant_context(plant_entry)
            system_prompt = PromptTemplates.general_with_plant(context_from_metadata)
            # We have local data, so no web search is needed.
            return self.llm_service.generate_response(
                question,
                system_prompt,
                history=conversation.get_history_for_llm(),
                allow_web_search=False
            )
        else:
            # If even the direct lookup fails, resort to web search.
            print(f"‚ö†Ô∏è Metadata lookup also failed for '{label}'. Resorting to web search.")
            return self._fallback_to_web_search(f"Th√¥ng tin v·ªÅ c√¢y {label}: {question}", conversation)


    def handle_medical_question_with_label(self, label: str, question: str, conversation: ConversationManager) -> str:
        """Handle medical questions about a specific plant"""
        print(f"üè• Medical question about {label}")
        
        context_parts = []
        
        # Get basic plant info
        plant_data = self.plant_service.find_plant_by_name(label)
        if plant_data:
            plant_context = self.plant_service.build_plant_context(plant_data)
            context_parts.append("TH√îNG TIN C√ÇY C·ª§ TH·ªÇ:")
            context_parts.append(plant_context)
            context_parts.append("")
            print("‚úÖ Added basic plant metadata")
        
        # Get RAG medical context
        if self.use_rag and self.rag_system:
            try:
                rag_result = self.rag_system.query(f"Th√¥ng tin y h·ªçc v·ªÅ c√¢y {label}: {question}")
                if rag_result.get('search_results_count', 0) > 0:
                    rag_context = rag_result.get('context_for_llm', '')
                    context_parts.append("TH√îNG TIN Y H·ªåC & T∆Ø∆†NG T√ÅC (T·ª™ H·ªÜ TH·ªêNG RAG):")
                    context_parts.append(rag_context)
                    context_parts.append("")
                    print(f"‚úÖ Enhanced with RAG context ({rag_result['search_results_count']} sources)")
                else:
                    print("‚ö†Ô∏è RAG found no specific medical info for this plant")
            except Exception as e:
                print(f"‚ö†Ô∏è RAG enhancement failed: {e}")
        
        # Build full context text
        full_context_text = "\n".join(context_parts)
        
        # Generate response (no web search - use only provided context)
        system_prompt = PromptTemplates.medical_with_plant(label, full_context_text)
        return self.llm_service.generate_response(
            question, 
            system_prompt,
            history=conversation.get_history_for_llm(),
            allow_web_search=False
        )
    
    def handle_medical_question_without_label(self, question: str, conversation: ConversationManager) -> str:
        """Handle general medical questions using RAG"""
        print("üè• General medical question")
        
        context_for_llm = ""
        if self.use_rag and self.rag_system:
            try:
                rag_result = self.rag_system.query(question)
                if rag_result.get('search_results_count', 0) > 0:
                    context_for_llm = rag_result.get('context_for_llm', '')
                    print(f"‚úÖ RAG context added for medical question {context_for_llm}...")
                    print(f"‚úÖ RAG found {rag_result['search_results_count']} relevant plants")
                else:
                    print("‚ö†Ô∏è RAG found no relevant plants for this medical query")
            except Exception as e:
                print(f"‚ö†Ô∏è RAG error: {e}")
        
        # Generate response (no web search - use only RAG context)
        system_prompt = PromptTemplates.medical_general(context_for_llm)
        return self.llm_service.generate_response(
            question,
            system_prompt, 
            history=conversation.get_history_for_llm(),
            allow_web_search=False
        )
    
    def handle_general_question_with_metadata(self, label: str, question: str, conversation: ConversationManager) -> str:
        """Handle non-medical questions about a specific labeled plant, leveraging its metadata."""
        print(f"üìñ Standard information question about {label}")
        
        plant_entry = self.plant_service.search_by_name(self.plant_service.metadata, label)
        
        context_from_metadata = ""
        if plant_entry:
            print("‚úÖ Found plant metadata.")
            plant_metadata_info = plant_entry
            for key, value in plant_metadata_info.items():
                context_from_metadata += f"{key}: {value}\n"
        else:
            print(f"‚ö†Ô∏è No metadata found for labeled plant '{label}'. Attempting web search for general info.")
        
        system_prompt = PromptTemplates.general_with_plant(context_from_metadata)
        
        return self.llm_service.generate_response(
            question,
            system_prompt,
            history=conversation.get_history_for_llm(),
            allow_web_search=False if plant_entry else True  # Allow web search only if no metadata found
        )
    
    def handle_general_question(self, question: str, conversation: ConversationManager) -> str:
        """Handle general questions without specific plant"""
        print("üí¨ General question without specific plant")
        return self.llm_service.generate_response(
            question,
            PromptTemplates.GENERAL_NO_PLANT,
            history=conversation.get_history_for_llm(),
            allow_web_search=True
        )
    
    def generate_answer(self, label: Optional[str], question: str, session_id: str = None) -> str:
        """
        Main answer generation with a RAG-first strategy and intelligent fallbacks.
        """
        session_id, conversation = self.session_manager.get_or_create_session(session_id)
        
        if question.strip().lower() == 'reset':
            print(f"üîÑ Received 'reset' command for session {session_id[:8]}. Clearing history.")
            self.reset_conversation(session_id)
            
            # Create a confirmation message
            answer = "ƒê√£ ƒë·∫∑t l·∫°i cu·ªôc tr√≤ chuy·ªán. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu l·∫°i t·ª´ ƒë·∫ßu."
            
            # Add only the confirmation to the now-empty history
            conversation.add_message("model", answer)
            return answer
        
        print(f"üí¨ Processing question in session {session_id[:8]}...")
        conversation.add_message("user", question)
        
        meta_answer = conversation.handle_meta_questions(question, self.plant_service)
        if meta_answer:
            conversation.add_message("model", meta_answer)
            return meta_answer

        answer = None

        # --- RAG-First Strategy ---
        if self.use_rag:
            # Try to answer using the powerful RAG system first.
            answer = self._handle_question_with_rag(question, conversation, label)

        # --- Fallback Logic ---
        # If RAG did not produce an answer (returned None), use fallback methods.
        if answer is None:
            print("- RAG did not provide an answer. Initiating fallback logic. -")
            if label:
                # If we have a label, our best fallback is a direct metadata lookup.
                answer = self._fallback_to_metadata_lookup(label, question, conversation)
            else:
                # If we have no label and RAG failed, the only option is web search.
                answer = self._fallback_to_web_search(question, conversation)
        
        conversation.add_message("model", answer)
        return answer

    
    def reset_conversation(self, session_id: str = None):
        """Reset a specific session's conversation history"""
        if session_id:
            session_id, conversation = self.session_manager.get_or_create_session(session_id)
            conversation.history = []
            print(f"üîÑ Reset conversation history for session {session_id[:8]}...")
        else:
            print("‚ö†Ô∏è No session_id provided for conversation reset")
    
    def close(self):
        """Clean up resources"""
        if self.rag_system:
            self.rag_system.clear_cache()
            self.rag_system = None
        print(f"PlantNLPSystem: Resources cleaned up ({self.session_manager.get_session_count()} active sessions)")


# Backward compatibility wrapper
class PlantQA(PlantNLPSystem):
    """Backward compatibility wrapper"""
    
    def __init__(self, metadata_path: str = RAGConfig.PLANT_METADATA_JSON_PATH, 
                 graph_path=RAGConfig.PLANT_GRAPH_PATH, db_service=None, cache_dir = None):
        # Ignore db_service parameter for compatibility
        super().__init__(metadata_path, graph_path, cache_dir=cache_dir)
        # Store for legacy compatibility
        self.metadata = self.plant_service.metadata
    
    def convert(self, value):
        """Legacy method for name mapping"""
        name_mapping_file = '/home/sora/code/name_mapping.json'
        if os.path.exists(name_mapping_file):
            with open(name_mapping_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                for key, val in metadata.items():
                    if val == value:
                        return key
        return None